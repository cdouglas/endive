# Experiment 5.1: Single Table with Varying Catalog Latency - False Conflicts
#
# Research Question: How does catalog latency affect throughput ceiling?
#
# Setup:
# - Single table (pure serialization, no multi-table effects)
# - All conflicts are "false" (version changed but no overlapping data)
# - **SWEEP CATALOG LATENCY** from 15ms to 1000ms
# - Realistic storage latencies for manifest lists/files (50ms)
#
# Hypothesis:
# - Throughput ceiling inversely proportional to catalog latency
# - Fast catalog (15ms) → ~66 txn/sec ceiling
# - Slow catalog (1000ms) → ~1 txn/sec ceiling
#
# Expected Results:
# - With fast catalog: conflict resolution costs visible
# - With slow catalog: catalog latency dominates everything
# - Clear linear relationship between catalog latency and max throughput
#
# How to Run:
# Sweep both catalog latency and load:
#   T_CAS.mean = [15, 50, 100, 200, 500, 1000] ms
#   inter_arrival.scale = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000]
#
# For each combination, run multiple seeds:
#   python -m icecap.main experiment_configs/exp5_1_single_table_catalog_latency.toml

[simulation]
duration_ms = 3600000  # 1 hour (extended for stable statistics)
output_path = "results.parquet"
# seed = 42  # Uncomment to set specific seed, or leave unset for random

[experiment]
label = "exp5_1_single_table_catalog_latency"

[catalog]
num_tables = 1
num_groups = 1  # Single group (all tables in one group)

[transaction]
retry = 10

# Transaction runtime (lognormal distribution)
# Realistic Iceberg transaction durations: 30s minimum, 3min mean
runtime.min = 30000   # 30 seconds
runtime.mean = 180000  # 3 minutes
runtime.sigma = 1.5

# Inter-arrival time distribution
# **SWEEP THIS PARAMETER** to vary offered load
inter_arrival.distribution = "exponential"
inter_arrival.scale = 500.0  # ~2 txn/sec (MODIFY THIS FOR LOAD SWEEP)

# Single-table workload
ntable.zipf = 10.0  # Very high = always 1 table
seltbl.zipf = 1.0   # Uniform (doesn't matter with 1 table)
seltblw.zipf = 1.0  # All read tables are written

# Conflict resolution: FALSE CONFLICTS ONLY
real_conflict_probability = 0.0  # All conflicts are false

# For real conflicts (not used in this experiment)
conflicting_manifests.distribution = "exponential"
conflicting_manifests.mean = 3.0
conflicting_manifests.min = 1
conflicting_manifests.max = 10

[storage]
# Parallelism limit for manifest operations during conflict resolution
max_parallel = 4

# Minimum latency for any storage operation (ms)
min_latency = 1

# ===== CATALOG LATENCY (SWEEP THIS) =====
# **SWEEP THIS PARAMETER** to study catalog latency impact
T_CAS.mean = 50  # MODIFY THIS: 15, 50, 100, 200, 500, 1000
T_CAS.stddev = 5  # MODIFY THIS: 10% of mean

# Metadata root latency (same as CAS - both are catalog operations)
T_METADATA_ROOT.read.mean = 50   # MODIFY THIS: match T_CAS.mean
T_METADATA_ROOT.read.stddev = 5  # MODIFY THIS: 10% of mean
T_METADATA_ROOT.write.mean = 50  # MODIFY THIS: match T_CAS.mean
T_METADATA_ROOT.write.stddev = 5 # MODIFY THIS: 10% of mean

# ===== REALISTIC STORAGE LATENCIES =====
# Typical S3 performance (kept constant)
T_MANIFEST_LIST.read.mean = 50
T_MANIFEST_LIST.read.stddev = 5
T_MANIFEST_LIST.write.mean = 60
T_MANIFEST_LIST.write.stddev = 6

T_MANIFEST_FILE.read.mean = 50
T_MANIFEST_FILE.read.stddev = 5
T_MANIFEST_FILE.write.mean = 60
T_MANIFEST_FILE.write.stddev = 6
