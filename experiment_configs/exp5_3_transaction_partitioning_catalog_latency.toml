# Experiment 5.3: Transaction Partitioning with Varying Catalog Latency
#
# Research Question: How does transaction partitioning interact with catalog latency?
#
# Setup:
# - Fixed number of tables (20)
# - **SWEEP NUM_GROUPS** from 1 (catalog-level conflicts) to 20 (table-level conflicts)
# - **SWEEP CATALOG LATENCY** from 15ms to 1000ms
# - All conflicts are "false" (version changed but no overlapping data)
# - Realistic storage latencies for manifest lists/files
#
# Hypothesis:
# - With fast catalog (15ms): More groups significantly reduces contention
# - With slow catalog (1000ms): Catalog latency dominates, grouping matters less
# - Crossover point: Where catalog speed overtakes contention as bottleneck
#
# Expected Results:
# - Fast catalog: Partitioning effectiveness high (reduces hot-table contention)
# - Slow catalog: Partitioning effectiveness low (catalog is already bottleneck)
# - This reveals when transaction partitioning is worth the complexity
#
# How to Run:
# Sweep catalog latency, num_groups, and load:
#   T_CAS.mean = [15, 50, 100, 200, 500, 1000] ms
#   num_groups = [1, 2, 5, 10, 20]
#   inter_arrival.scale = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000]
#
# Note: num_tables is fixed at 20 to allow meaningful group variations
#
# For each combination, run multiple seeds:
#   python -m endive.main experiment_configs/exp5_3_transaction_partitioning_catalog_latency.toml

[simulation]
duration_ms = 3600000  # 1 hour (extended for stable statistics)
output_path = "results.parquet"
# seed = 42  # Uncomment to set specific seed, or leave unset for random

[experiment]
label = "exp5_3_transaction_partitioning_catalog_latency"

[catalog]
# Fixed at 20 tables to allow meaningful group variations
num_tables = 20

# **SWEEP THIS PARAMETER** to study transaction partitioning
# 1 = catalog-level conflicts (all txns contend)
# 20 = table-level conflicts (minimal contention)
# 2,5,10 = intermediate partitioning
num_groups = 5  # MODIFY THIS: 1, 2, 5, 10, 20

[transaction]
retry = 10

# Transaction runtime (lognormal distribution)
# Realistic Iceberg transaction durations: 30s minimum, 3min mean
runtime.min = 30000   # 30 seconds
runtime.mean = 180000  # 3 minutes
runtime.sigma = 1.5

# Inter-arrival time distribution
# **SWEEP THIS PARAMETER** to vary offered load
inter_arrival.distribution = "exponential"
inter_arrival.scale = 500.0  # ~2 txn/sec (MODIFY THIS FOR LOAD SWEEP)

# Multi-table workload: transactions access 2-3 tables on average
ntable.zipf = 1.5  # Lower = more tables per transaction
seltbl.zipf = 1.4  # Which tables selected (lower = more uniform)
seltblw.zipf = 1.2  # Tables written vs read

# Conflict resolution: FALSE CONFLICTS ONLY
real_conflict_probability = 0.0  # All conflicts are false

# For real conflicts (not used in this experiment)
conflicting_manifests.distribution = "exponential"
conflicting_manifests.mean = 3.0
conflicting_manifests.min = 1
conflicting_manifests.max = 10

[storage]
# Parallelism limit for manifest operations during conflict resolution
max_parallel = 4

# Minimum latency for any storage operation (ms)
min_latency = 1

# ===== CATALOG LATENCY (SWEEP THIS) =====
# **SWEEP THIS PARAMETER** to study catalog latency impact
T_CAS.mean = 50  # MODIFY THIS: 15, 50, 100, 200, 500, 1000
T_CAS.stddev = 5  # MODIFY THIS: 10% of mean

# Metadata root latency (same as CAS - both are catalog operations)
T_METADATA_ROOT.read.mean = 50   # MODIFY THIS: match T_CAS.mean
T_METADATA_ROOT.read.stddev = 5  # MODIFY THIS: 10% of mean
T_METADATA_ROOT.write.mean = 50  # MODIFY THIS: match T_CAS.mean
T_METADATA_ROOT.write.stddev = 5 # MODIFY THIS: 10% of mean

# ===== REALISTIC STORAGE LATENCIES =====
# Typical S3 performance (kept constant)
T_MANIFEST_LIST.read.mean = 50
T_MANIFEST_LIST.read.stddev = 5
T_MANIFEST_LIST.write.mean = 60
T_MANIFEST_LIST.write.stddev = 6

T_MANIFEST_FILE.read.mean = 50
T_MANIFEST_FILE.read.stddev = 5
T_MANIFEST_FILE.write.mean = 60
T_MANIFEST_FILE.write.stddev = 6
