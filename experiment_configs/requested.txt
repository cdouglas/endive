Requested:

Let's turn now to the analysis. Our goal is to characterize the workloads that
could (not) be supported by a storage-only catalog and why. To start, we want
to establish a baseline for an infinitely fast catalog.

1)  When does the commit rate of a single table saturate given an infinitely
    fast catalog? Running a workload against a single table, we want the following
    latency vs throughput graphs (each point annotated with % successful txn):

1a) All false conflicts i.e., the query engine never needs to rewrite manifest
    files or update deletion vectors. The only costs are reading manifest lists
    and reading/writing the new manifest list and table manifest. We want to
    increase the offered load until the abort rate exceeds 50%

1b) Repeat 1a, but increase the probability of actual conflicts (requiring more
    steps for the engine to retry, increasing the retry latency). The number
    of conflicting manifests should be drawn from a distribution.

2) For N tables, when does the infinitely fast catalog saturate with
    multi-table transactions? Redraw the latency vs throughput graphs from 1a
    with these variations.

2a) All false conflicts. None of the tables in the multi-table transaction
    have real conflicts, but manifest lists from both tables need to be read
    while respecting a single parallel read parameter.

2b) Some real conflicts. The probability of a real conflict is increased because
    more than one table could require repair by the query engine.

Develop a plan to use the simulator to explore these questions. Critically
examine whether the goals are well formed and whether the current simulator is
equipped to answer the questions posed. Suggest other analyses that would
elucidate the broader goal of understanding the fundamental per-table limits of
Iceberg transaction throughput.

