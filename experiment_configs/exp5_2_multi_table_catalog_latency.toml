# Experiment 5.2: Multi-Table with Varying Catalog Latency - False Conflicts
#
# Research Question: Does catalog latency change the multi-table scaling behavior?
#
# Setup:
# - Multiple tables with table-level conflicts (num_groups = num_tables)
# - All conflicts are "false" (version changed but no overlapping data)
# - **SWEEP CATALOG LATENCY** from 15ms to 1000ms
# - **SWEEP TABLE COUNT** to see if hot-table effects change with slow catalog
# - Realistic storage latencies for manifest lists/files
#
# Hypothesis:
# - With fast catalog (15ms): hot table bottleneck dominates (as in exp 2.2)
# - With slow catalog (1000ms): catalog latency dominates, table count matters less
# - Crossover point where catalog latency overtakes hot-table contention
#
# Expected Results:
# - Fast catalog: throughput similar across table counts (hot table bottleneck)
# - Slow catalog: more tables helps slightly (less contention per CAS)
#
# How to Run:
# Sweep catalog latency, load, and table count:
#   T_CAS.mean = [15, 50, 100, 200, 500, 1000] ms
#   num_tables = [1, 5, 20, 50]
#   inter_arrival.scale = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000]
#
# IMPORTANT: Set num_groups = num_tables for table-level conflicts
#
# For each combination, run multiple seeds:
#   python -m icecap.main experiment_configs/exp5_2_multi_table_catalog_latency.toml

[simulation]
duration_ms = 3600000  # 1 hour (extended for stable statistics)
output_path = "results.parquet"
# seed = 42  # Uncomment to set specific seed, or leave unset for random

[experiment]
label = "exp5_2_multi_table_catalog_latency"

[catalog]
# **SWEEP THIS PARAMETER** to study scaling with table count
num_tables = 10  # MODIFY THIS: 1, 5, 20, 50

# IMPORTANT: Set num_groups = num_tables for table-level conflicts
num_groups = 10  # MODIFY THIS: must equal num_tables

[transaction]
retry = 10

# Transaction runtime (lognormal distribution)
# Realistic Iceberg transaction durations: 30s minimum, 3min mean
runtime.min = 30000   # 30 seconds
runtime.mean = 180000  # 3 minutes
runtime.sigma = 1.5

# Inter-arrival time distribution
# **SWEEP THIS PARAMETER** to vary offered load
inter_arrival.distribution = "exponential"
inter_arrival.scale = 500.0  # ~2 txn/sec (MODIFY THIS FOR LOAD SWEEP)

# Multi-table workload: transactions access 2-3 tables on average
ntable.zipf = 1.5  # Lower = more tables per transaction
seltbl.zipf = 1.4  # Which tables selected (lower = more uniform)
seltblw.zipf = 1.2  # Tables written vs read

# Conflict resolution: FALSE CONFLICTS ONLY
real_conflict_probability = 0.0  # All conflicts are false

# For real conflicts (not used in this experiment)
conflicting_manifests.distribution = "exponential"
conflicting_manifests.mean = 3.0
conflicting_manifests.min = 1
conflicting_manifests.max = 10

[storage]
# Parallelism limit for manifest operations during conflict resolution
max_parallel = 4

# Minimum latency for any storage operation (ms)
min_latency = 1

# ===== CATALOG LATENCY (SWEEP THIS) =====
# **SWEEP THIS PARAMETER** to study catalog latency impact
T_CAS.mean = 50  # MODIFY THIS: 15, 50, 100, 200, 500, 1000
T_CAS.stddev = 5  # MODIFY THIS: 10% of mean

# Metadata root latency (same as CAS - both are catalog operations)
T_METADATA_ROOT.read.mean = 50   # MODIFY THIS: match T_CAS.mean
T_METADATA_ROOT.read.stddev = 5  # MODIFY THIS: 10% of mean
T_METADATA_ROOT.write.mean = 50  # MODIFY THIS: match T_CAS.mean
T_METADATA_ROOT.write.stddev = 5 # MODIFY THIS: 10% of mean

# ===== REALISTIC STORAGE LATENCIES =====
# Typical S3 performance (kept constant)
T_MANIFEST_LIST.read.mean = 50
T_MANIFEST_LIST.read.stddev = 5
T_MANIFEST_LIST.write.mean = 60
T_MANIFEST_LIST.write.stddev = 6

T_MANIFEST_FILE.read.mean = 50
T_MANIFEST_FILE.read.stddev = 5
T_MANIFEST_FILE.write.mean = 60
T_MANIFEST_FILE.write.stddev = 6
